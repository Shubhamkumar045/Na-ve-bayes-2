{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635c6ad-4af1-4954-8287-ea78608fa3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "Answer--To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem.\n",
    "\n",
    "Let's define the events:\n",
    "\n",
    "�\n",
    "S: The event that an employee is a smoker.\n",
    "�\n",
    "H: The event that an employee uses the health insurance plan.\n",
    "We are given:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(H), the probability that an employee uses the health insurance plan, which is 70% or 0.70.\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(S∣H), the probability that an employee is a smoker given that he/she uses the health insurance plan, which is 40% or 0.40.\n",
    "We need to find \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(S∣H), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "×\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(S∣H)= \n",
    "P(H)\n",
    "P(H∣S)×P(S)\n",
    "​\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "Answer--\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes\n",
    "algorithm that are commonly used for text classification tasks, but they have different \n",
    "assumptions and are suitable for different types of data.\n",
    "\n",
    "Here are the key differences between Bernoulli Naive Bayes and Multinomial Naive Bayes:\n",
    "\n",
    "Nature of Features:\n",
    "\n",
    "Bernoulli Naive Bayes: Assumes that features are binary (e.g., presence or absence of a\n",
    "                                                         term in a document).\n",
    "Multinomial Naive Bayes: Assumes that features represent counts or frequencies of terms\n",
    "(e.g., the number of times a term appears in a document).\n",
    "Feature Representation:\n",
    "\n",
    "Bernoulli Naive Bayes: Typically used when working with binary feature vectors, such as \n",
    "document term presence/absence matrices.\n",
    "Multinomial Naive Bayes: Typically used when working with term frequency-based feature\n",
    "vectors, such as bag-of-words representations.\n",
    "Handling Zero Counts:\n",
    "\n",
    "Bernoulli Naive Bayes: Ignores the number of occurrences of a term beyond 1 (presence/absence only).\n",
    "Multinomial Naive Bayes: Takes into account the frequency of occurrences of terms in the document.\n",
    "Use Cases:\n",
    "\n",
    "Bernoulli Naive Bayes: Often used for text classification tasks where presence or absence \n",
    "of terms is more important than their frequency, such as sentiment analysis or spam detection.\n",
    "Multinomial Naive Bayes: Commonly used for text classification tasks where term frequencies \n",
    "or counts are crucial, such as document categorization or topic modeling.\n",
    "Parameter Estimation:\n",
    "\n",
    "Bernoulli Naive Bayes: Typically uses maximum likelihood estimation to estimate the parameters\n",
    "(probabilities) of the model.\n",
    "Multinomial Naive Bayes: Also uses maximum likelihood estimation for parameter estimation, \n",
    "but it estimates the probability of observing each term in each class.\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "Answer--\n",
    "Bernoulli Naive Bayes, like other variants of the Naive Bayes algorithm, typically \n",
    "assumes that all features are binary, representing the presence or absence of a \n",
    "particular term or attribute. Therefore, in the context of Bernoulli Naive Bayes,\n",
    "missing values can be handled in different ways:\n",
    "\n",
    "Imputation:\n",
    "\n",
    "One common approach is to impute missing values before applying the Bernoulli Naive\n",
    "Bayes algorithm. Imputation involves replacing missing values with estimated or \n",
    "inferred values. For binary features, missing values may be imputed with the\n",
    "absence of the corresponding term or attribute.\n",
    "For example, in text classification tasks, missing values (i.e., missing terms)\n",
    "in documents can be imputed by assuming that the missing terms are absent from the document.\n",
    "Handling Missing Values as a Separate Category:\n",
    "\n",
    "Another approach is to treat missing values as a separate category or state of\n",
    "the feature. In the context of Bernoulli Naive Bayes, this may involve explicitly\n",
    "modeling missing values as a binary feature alongside the presence or absence of \n",
    "other terms or attributes.\n",
    "For example, missing values may be represented as a binary indicator variable where\n",
    "1 indicates the presence of a missing value and 0 indicates the absence of a missing value.\n",
    "Ignoring Missing Values:\n",
    "\n",
    "In some cases, missing values may be ignored or excluded from the analysis. This\n",
    "approach may be appropriate if the proportion of missing values is small and not \n",
    "expected to significantly impact the results of the Bernoulli Naive Bayes algorithm.\n",
    "However, ignoring missing values without considering their potential impact on the\n",
    "analysis may lead to biased or inaccurate results, especially if the missing values \n",
    "are not missing completely at random (MCAR) or missing at random (MAR).\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "Answer--Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. \n",
    "Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes continuous features follow a Gaussian distribution. It is particularly well-suited for data with continuous features.\n",
    "\n",
    "For multi-class classification, Gaussian Naive Bayes applies the standard Naive Bayes framework to estimate the posterior probability of each class given the observed feature values. The class with the highest posterior probability is then predicted as the outcome.\n",
    "\n",
    "The algorithm estimates the parameters (mean and variance) of the Gaussian distribution for each feature within each class. Given a new instance with feature values, it calculates the likelihood of observing those feature values given each class and combines it with the prior probability of each class to compute the posterior probabilities using Bayes' theorem.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes calculates the posterior probabilities for each class, and the class with the highest posterior probability is assigned as the predicted class for the given instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
